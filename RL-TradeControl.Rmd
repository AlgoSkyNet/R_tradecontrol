---
title: "RL"
output: html_notebook
author: Vladimir Zhbanko
date: 2018-01-30
---

# Reinforcement Learning

Implementing [vignette](https://cran.r-project.org/web/packages/ReinforcementLearning/vignettes/ReinforcementLearning.html)

## Goal

Study RL to create a system/algorithm that is capable to automatically decide whether to trade or not to trade... After moving to each new state system would recieve the response after completing state called Reward. It will learn to take decision based on trial and error approach... Method called Q-Learning...

### States

| ---- | s1 | <----> | s2 | ---- | 

- s1 - trade
- s2 - no-trade

### Actions

Actions of our trading system will be the two:

- "right"
- "left"
- "stay"

Where *ON* means that trading system will stay ON
and **OFF* will mean that trading system will stay OFF

by default however our trading system will always trade. It would be necessary to create some 'limiting' or 'environment' function that would stop this system to trade. This limiting function would be derived calculating the profit factor

### Rewards

Algorithm would have several rewards levels:

- 0 when no trade is done
- 10 when trade was winning
- -4 when trade was loosing

Ideally to convert profit / loss to our reward and normalize it from 10 to -10

### Next State

see states

### Environment Function

We would need Env.function in case we want to simulate the reinforcement learning. We would manually describe function behaviour. This function will recieve:

- state
- action

It will return:

- next state
- reward

We will not use this function in real trade but we will just be able to better learn the concepts...

```{r}
lazytradeEnvironment <- function(state, action){
  next_state <- state
  # States: 
  # "s1" tradewin
  # "s2" notrade
  # "s3" tradeloss
  # |-s1-|-s2-|-s3-|
  # Actions:
  # "right"
  # "left"
  # "stay"
  # 
  if(state == state("s1") && action == "right")
      next_state <- state("s2")
  if(state == state("s2") && action == "left")
      next_state <- state("s1")
  if(state == state("s2") && action == "right")
      next_state <- state("s3")
  if(state == state("s3") && action == "left")
      next_state <- state("s2")
  if(state == state("s1") && action == "stay")
      next_state <- state("s1")
  if(state == state("s2") && action == "stay")
      next_state <- state("s2")
  if(state == state("s3") && action == "stay")
      next_state <- state("s3")
  if (next_state == state("s1") && state != state("s1")) { # reward to win
        reward <- 10
  } else if(next_state == state("s3") && state != state("s3")){ # reward if loss
        reward <- -5
    }
    else {
        reward <- -1
    }
  
    
    
  output <- list(NextState = next_state, Reward = reward)
  return(output)
}

```

### Simulation of data for our trading problem

```{r}
library(ReinforcementLearning)
# Define state and action sets
states <- c("s1", "s2", "s3")
actions <- c("stay", "left", "right")

# Sample N = 1000 random sequences from the environment
data <- sampleExperience(N = 100, env = lazytradeEnvironment, states = states, actions = actions)
head(data)
```

### Perform our reinforcement learning

```{r}
# Define reinforcement learning parameters
control <- list(alpha = 0.1, gamma = 0.5, epsilon = 0.1)

# Perform reinforcement learning
model <- ReinforcementLearning(data, s = "State", a = "Action", r = "Reward", 
                               s_new = "NextState", control = control)

# Print result
print(model)
```

```{r}
policy(model)
```

### Update of policy

```{r}
# Define reinforcement learning parameters
control <- list(alpha = 0.1, gamma = 0.5, epsilon = 0.1)

# Sample N = 1000 sequences from the environment using epsilon-greedy action selection
data_new <- sampleExperience(N = 100, env = lazytradeEnvironment, states = states, actions = actions, 
                             model = model, actionSelection = "epsilon-greedy", 
                             control = control)
head(data_new)
```

```{r}
# Update the existing policy using new training data
model_new <- ReinforcementLearning(data_new, s = "State", a = "Action", r = "Reward", 
                                   s_new = "NextState", control = control, model = model)

# Print result
print(model_new)
```

```{r}
# Plot reinforcement learning curve
plot(model_new)
```
```{r}
summary(model_new)
```


## Adapt data for RL problem

For our reinforcement learning data will not be coming from simulation. Data will be coming from trading results. States of our system can be *tradewin* or *tradeloss* and we can also have 2 possible actions: *ON*/*OFF*.

'Past' data will need to be converted into the following elements as follows:

--- Iteration 1 ---
At time 1:

- state: tradewin
- action: ON

At time 2:

- next state: tradeloss
- reward: -1

--- Iteration 2 ---
At time 2:

- state: tradeloss
- action: OFF

At time 3:

- next state: tradewin
- reward: 12

Data needs to be formatted in this way:

```{r}
data_time1 <- data.frame(State = "tradewin", Action = "ON", Reward = -1, NextState = "tradeloss",
                        stringsAsFactors = F)
data_time2 <- data.frame(State = "tradeloss", Action = "OFF", Reward = 12, NextState = "tradewin",
                        stringsAsFactors = F)
data_test <- rbind(data_time1, data_time2)
print(data_test)
```

Reward will correspond to the actual profit or loss. We will be continously feeding data after every new trade of that system. Model will generate action that will be used in our 'other' terminal...

The idea will be to actually feed fresh data and always give to the system last 10 rows of results (11 trades)

In the next paragraph we would convert this data to be used by RL

### Data Preparation and RL

The approach of data preparation will be the same as for Trading Monitoring. Potentially there will be a need to write specific function.
Overall data manipulation should follow this path:

1). Read data from Sandbox
2). Clean data by date
3). Group by Trading System
4). Order data
4). Extract maximum 11 latest rows of each system
5). use the last available row to generate hash of the vector
6). Perform reinforcement learning and build model if there is no model for that hash
7). Use action from model for the second terminal



```{r message=FALSE, warning=FALSE, paged.print=FALSE}
library(lubridate)
library(tidyverse)

# data from sandboxes
# specifying the path to the 4x terminals used into the dataframe
Terminals <- data.frame(id = 1:5, TermPath = c("C:/Program Files (x86)/FxPro - Terminal1/MQL4/Files/",
                                               "C:/Program Files (x86)/FxPro - Terminal2/MQL4/Files/",
                                               "C:/Program Files (x86)/FxPro - Terminal3/MQL4/Files/",
                                               "C:/Program Files (x86)/FxPro - Terminal4/MQL4/Files/",
                                               "C:/Program Files (x86)/FxPro - Terminal5/MQL4/Files/"),
                        stringsAsFactors = F)

sbox1 <- paste0(Terminals[1, 2], "OrdersResultsT", 1,".csv")
sbox2 <- paste0(Terminals[2, 2], "OrdersResultsT", 2,".csv")
sbox3 <- paste0(Terminals[3, 2], "OrdersResultsT", 3,".csv")
sbox4 <- paste0(Terminals[4, 2], "OrdersResultsT", 4,".csv")
```

Read data from sbox1. This terminal is used for our research and let our RL system to learn without risk...

```{r}
DF_sbox1 <- read_csv(file = sbox1, col_names = F)
DF_sbox1$X3 <- ymd_hms(DF_sbox1$X3)
DF_sbox1$X4 <- ymd_hms(DF_sbox1$X4)

DF_System1 <- DF_sbox1 %>% 
  group_by(X1) %>% 
  arrange(X1, desc(X4)) %>%
  filter(X1 == 8118101) %>% 
  tail(16)

DF_System1_n <- DF_sbox1 %>% 
  group_by(X1) %>% 
  arrange(X1, desc(X4)) %>%
  filter(X1 == 8118101) %>% 
  head(10)


head(DF_System1)
```

Now we have non trivial task to convert this data frame into something of completely different structure

```{r}
print(data_test)
```

Probably the best way to do is to experiment:

```{r}
DF1 <- DF_System1 %>% 
  # arrange as ascending
  arrange(X4) %>% 
  # create column State
  mutate(NextState = ifelse(X5>0, "tradewin",
                    ifelse(X5<0, "tradeloss", NA)),
         Action = ifelse(X5>0, "ON",
                    ifelse(X5<0, "OFF", NA)),
         Reward =  X5,
         State = lag(NextState)) %>% # State column will be shifted down
  select(X1, X4, State, Action, Reward, NextState)

DF1_n <- DF_System1_n %>% 
  # arrange as ascending
  arrange(X4) %>% 
  # create column State
  mutate(NextState = ifelse(X5>0, "tradewin",
                    ifelse(X5<0, "tradeloss", NA)),
         Action = ifelse(X5>0, "ON",
                    ifelse(X5<0, "OFF", NA)),
         Reward =  X5,
         State = lag(NextState)) %>% # State column will be shifted down
  select(X1, X4, State, Action, Reward, NextState)

```

This way we have a dataframe with proper structure. The oldest State is shifted and it's now with missing value:

```{r}
print(DF1)
```

Now we can remove this row and keep only columns that we will need...

```{r}
library(magrittr)
DF2 <- DF1 %>% 
  na.omit() %>% 
  arrange(desc(X4)) %>% 
  ungroup() %>% 
  select(State, Action, Reward, NextState) 

DF2n <- DF1_n %>% 
  na.omit() %>% 
  arrange(desc(X4)) %>% 
  ungroup() %>% 
  select(State, Action, Reward, NextState) 

```

Seems that we have our data for RL now... as agreed let's create hash value for our last row

```{r}
library(openssl)
recent_name <- DF2 %>% head(1) %>% as.character() %>% as.vector() %>% paste(collapse = "") %>% sha1()
print(recent_name)
```

## Implementation

```{r}
# package installation
#devtools::install_github("nproellochs/ReinforcementLearning")
library(ReinforcementLearning)
```


```{r}
# Define state and action sets
states <- c("tradeloss", "tradewin")
actions <- c("ON", "OFF")
```

### Reinforcement learning

data for RL must be provided in the dataframe `(s,a,r,s_new)`. It will be also required to provide column names for each elements in this dataframe

```{r}
# Define reinforcement learning parameters
control <- list(alpha = 0.1, gamma = 0.5, epsilon = 0.1)
```

```{r}
# Perform reinforcement learning
DF2 <- as.data.frame.data.frame(DF2)
model <- ReinforcementLearning(DF2, s = "State", a = "Action", r = "Reward", s_new = "NextState",iter = 2, control = control)
```
 
model is ready

The result of the process provide best state-action table with optimal policy

```{r}
# Print result
print(model)
```

we can print also best possible policy:
```{r}
policy(model)
```

## Updating Existing Policy with New data

Update model with new data containing the same policy

```{r}
# Update the existing policy using new training data
DF2n <- as.data.frame.data.frame(DF2n)
model_new <- ReinforcementLearning(DF2n, s = "State", a = "Action", r = "Reward", 
                                   s_new = "NextState", control = control, iter = 3, model = model)

# Print result
print(model_new)
```

Now the reward is much higher!

```{r}
summary(model_new)
```

```{r}
policy(model_new)
```


```{r}
plot(model_new)
```

## Writing Function (productionizing)

Model in theory is good but we will still need to productionize and calculate 'rolling' profit factor as well. Some sort of workflow block diagram will need to be written first...

