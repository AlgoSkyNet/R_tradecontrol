---
title: "RL"
output: html_notebook
author: Vladimir Zhbanko
date: 2018-01-30
---

# Reinforcement Learning

Implementing [vignette](https://cran.r-project.org/web/packages/ReinforcementLearning/vignettes/ReinforcementLearning.html)

## Goal

Study RL to create a system/algorithm that is capable to automatically decide whether to trade or not to trade... After moving to each new state system would recieve the response after completing state. It will learn to take decision based on trial and error approach...

| ---- | s1 | ---- | s2 | ---- | 

System would initially stay in state s1. It would compute probabilities of winning in new state s2 based on the past results.

'Past' results will be continously loaded to the system by the completed trades in the Demo account. This way system will continously learn to guess what to do...

## Example of Implementation

```{r}
# package installation
#devtools::install_github("nproellochs/ReinforcementLearning")
library(ReinforcementLearning)
```

### Data format

```{r}
data("tictactoe")
head(tictactoe, 5)

```

All the data needs to be formulated in this dataframe.

### Dynamics of environment

Is defined by the function which needs to be written for each situation. Sample function is available:

` |———–-----|`
` | s1 | s4 |`
` | s2   s3 |`
` |———–-----|`

```{r}
env <- gridworldEnvironment
print(env)
```

This function needs to be modified for the current problem of RL

```{r}
# Define state and action sets
states <- c("s1", "s2", "s3", "s4")
actions <- c("up", "down", "left", "right")
```

Apparently it's possible to sample 'sequences from experience'

```{r}
# Sample N = 1000 random sequences from the environment
set.seed(69)
data <- sampleExperience(N = 1000, env = env, states = states, actions = actions)
head(data)
```

in this example we can see that passing from state s3 with action 'up' will result in stete s4 with reward 10

### Reinforcement learning

data for RL must be provided in the dataframe `(s,a,r,s_new)`. It will be also required to provide column names for each elements in this dataframe

```{r}
# Define reinforcement learning parameters
control <- list(alpha = 0.1, gamma = 0.5, epsilon = 0.1)
```

```{r}
# Perform reinforcement learning
model <- ReinforcementLearning(data, s = "State", a = "Action", r = "Reward", 
                               s_new = "NextState", control = control)
```
 
model is ready

The result of the process provide best state-action table with optimal policy

```{r}
# Print result
print(model)
```

we can print also best possible policy:
```{r}
policy(model)
```

## Updating Existing Policy with New data



```{r}
# Define reinforcement learning parameters
control <- list(alpha = 0.1, gamma = 0.5, epsilon = 0.1)

# Sample N = 1000 sequences from the environment using epsilon-greedy action selection
data_new <- sampleExperience(N = 1000, env = env, states = states, actions = actions, 
                             model = model, actionSelection = "epsilon-greedy", 
                             control = control)

head(data_new)

```

Update model with new data containing the same policy

```{r}
# Update the existing policy using new training data
model_new <- ReinforcementLearning(data_new, s = "State", a = "Action", r = "Reward", 
                                   s_new = "NextState", control = control, model = model)

# Print result
print(model_new)
```

Now the reward is much higher!

```{r}
summary(model_new)
```

```{r}
plot(model_new)
```

## Parameters explained

"alpha" The learning rate, set between 0 and 1. Setting it to 0 means that the Q-values are never updated and, hence, nothing is learned. Setting a high value, such as 0.9, means that learning can occur quickly.

"gamma" Discount factor, set between 0 and 1. Determines the importance of future rewards. A factor of 0 will render the agent short-sighted by only considering current rewards, while a factor approaching 1 will cause it to strive for a greater reward over the long term.

"epsilon" Exploration parameter, set between 0 and 1. Defines the exploration mechanism in ϵϵ-greedy action selection. In this strategy, the agent explores the environment by selecting an action at random with probability ϵϵ. Alternatively, the agent exploits its current knowledge by choosing the optimal action with probability 1−ϵ1−ϵ. This parameter is only required for sampling new experience based on an existing policy.

"iter" Number of repeated learning iterations. Iter is an integer greater than 0. The default is set to 1.

